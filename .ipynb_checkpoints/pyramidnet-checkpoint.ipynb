{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49434e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "import tinygrad.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "371c7d29-7f40-4519-936d-790e608c2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock():\n",
    "    outchannel_ratio = 4\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "        # eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "        # https://docs.tinygrad.org/nn/#tinygrad.nn.BatchNorm\n",
    "        # eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "        self.bn1 = nn.BatchNorm(in_planes)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        # dilation=1, groups=1\n",
    "        # https://docs.tinygrad.org/nn/#tinygrad.nn.Conv2d\n",
    "        # dilation=1, groups=1\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm(planes)\n",
    "        self.relu = Tensor.relu\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = Tensor.zeros(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1])\n",
    "            out += Tensor.cat((shortcut, padding), 1)\n",
    "        else:\n",
    "            out += shortcut\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck():\n",
    "    outchannel_ratio = 4\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        self.bn1 = nn.BatchNorm(in_planes)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "        # stride=1, padding=0\n",
    "        # https://docs.tinygrad.org/nn/#tinygrad.nn.BatchNorm\n",
    "        # stride=1, padding=0\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.outchannel_ratio, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm(planes * Bottleneck.outchannel_ratio)\n",
    "        self.relu = Tensor.relu\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out = self.bn4(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = Tensor.zeros(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1])\n",
    "            out += Tensor.cat((shortcut, padding), 1)\n",
    "        else:\n",
    "            out += shortcut\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PyramidNet:\n",
    "    def __init__(self, in_planes, num_classes, depth, alpha, bottleneck=False):\n",
    "        if depth not in [18, 34, 50, 101, 152, 200]:\n",
    "            if bottleneck:\n",
    "                block = Bottleneck\n",
    "                temp_cfg = (depth - 2) // 12\n",
    "            else:\n",
    "                block = BasicBlock\n",
    "                temp_cfg = (depth - 2) // 8\n",
    "            layers = [temp_cfg, temp_cfg, temp_cfg, temp_cfg]\n",
    "            print('=> the layer configuration for each stage is set to', layers[depth])\n",
    "        else:\n",
    "            block = BasicBlock if depth <= 34 and not bottleneck else Bottleneck\n",
    "            if depth == 18:\n",
    "                layers = [2, 2, 2, 2]\n",
    "            elif depth in [34, 50]:\n",
    "                layers = [3, 4, 6, 3]\n",
    "            elif depth == 101:\n",
    "                layers = [3, 4, 23, 3]\n",
    "            elif depth == 152:\n",
    "                layers = [3, 8, 36, 3]\n",
    "            else:\n",
    "                layers = [3, 24, 36, 3]\n",
    "\n",
    "        self.in_planes = in_planes            \n",
    "        self.addrate = alpha / sum(layers)\n",
    "\n",
    "        self.input_featuremap_dim = self.in_planes\n",
    "        self.conv1 = nn.Conv2d(3, self.input_featuremap_dim, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.input_featuremap_dim)\n",
    "        self.relu = Tensor.relu\n",
    "\n",
    "        self.featuremap_dim = self.input_featuremap_dim \n",
    "        self.layer1 = self.pyramidal_make_layer(block, layers[0])\n",
    "        self.layer2 = self.pyramidal_make_layer(block, layers[1], stride=2)\n",
    "        self.layer3 = self.pyramidal_make_layer(block, layers[2], stride=2)\n",
    "        self.layer4 = self.pyramidal_make_layer(block, layers[3], stride=2)\n",
    "\n",
    "        self.final_featuremap_dim = self.input_featuremap_dim\n",
    "        self.bn_final = nn.BatchNorm2d(self.final_featuremap_dim)\n",
    "        self.relu_final = Tensor.relu\n",
    "        self.avgpool = lambda x: x.avg_pool2d(7)\n",
    "        self.fc = lambda x: x.linear(self.final_featuremap_dim, num_classes)\n",
    "\n",
    "    def pyramidal_make_layer(self, block, block_depth, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = lambda x: x.avg_pool2d((2, 2), stride=(2, 2)) # ceil_mode?\n",
    "\n",
    "        layers = []\n",
    "        self.featuremap_dim += self.addrate\n",
    "        layers.append(block(self.input_featuremap_dim, int(round(self.featuremap_dim)), stride, downsample))\n",
    "        for i in range(1, block_depth):\n",
    "            temp_featuremap_dim = self.featuremap_dim + self.addrate\n",
    "            layers.append(block(int(round(self.featuremap_dim)) * block.outchannel_ratio, int(round(temp_featuremap_dim)), 1))\n",
    "            self.featuremap_dim  = temp_featuremap_dim\n",
    "        self.input_featuremap_dim = int(round(self.featuremap_dim)) * block.outchannel_ratio\n",
    "        return layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.max_pool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        for layer in self.layer1:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer2:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer3:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer4:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.bn_final(x)\n",
    "        x = self.relu_final(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.flatten()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7c47d20-2439-4d9a-b33f-8d43455d8565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "dict_values([16, 6.0, 256, <tinygrad.nn.Conv2d object at 0x000001A7FFC300E0>, <tinygrad.nn.BatchNorm object at 0x000001A7A3279D00>, <function Tensor.relu at 0x000001A7D21AA840>, 64.0, [<__main__.BasicBlock object at 0x000001A7A32783B0>, <__main__.BasicBlock object at 0x000001A7A327BA10>], [<__main__.BasicBlock object at 0x000001A7A3CA2E40>, <__main__.BasicBlock object at 0x000001A7A3BF6270>], [<__main__.BasicBlock object at 0x000001A7A3C516A0>, <__main__.BasicBlock object at 0x000001A7A3C00A10>], [<__main__.BasicBlock object at 0x000001A7A3C03E00>, <__main__.BasicBlock object at 0x000001A7A3C63230>], 256, <tinygrad.nn.BatchNorm object at 0x000001A7A3C6A660>, <function Tensor.relu at 0x000001A7D21AA840>, <function PyramidNet.__init__.<locals>.<lambda> at 0x000001A7A4F8BEC0>, <function PyramidNet.__init__.<locals>.<lambda> at 0x000001A7A325C220>])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "optimizer must have at least one param",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 43\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m SGD([param \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, Tensor)], lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Loss function\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy_loss\u001b[39m(logits, labels):\n",
      "File \u001b[1;32m~\\fedml-miniconda\\envs\\tinygrad\\Lib\\site-packages\\tinygrad\\nn\\optim.py:65\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(params, lr, momentum, weight_decay, nesterov, classic)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mSGD\u001b[39m(params: List[Tensor], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, classic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m  Stochastic Gradient Descent (SGD) optimizer with optional momentum and weight decay.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m  - Described: https://paperswithcode.com/method/sgd\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m LARS(params, lr, momentum, weight_decay, nesterov, classic, tcoef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[1;32m~\\fedml-miniconda\\envs\\tinygrad\\Lib\\site-packages\\tinygrad\\nn\\optim.py:75\u001b[0m, in \u001b[0;36mLARS.__init__\u001b[1;34m(self, params, lr, momentum, weight_decay, nesterov, classic, tcoef)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params:List[Tensor], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, classic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tcoef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m):\n\u001b[1;32m---> 75\u001b[0m   \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, lr)\n\u001b[0;32m     76\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnesterov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassic, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcoef \u001b[38;5;241m=\u001b[39m momentum, weight_decay, nesterov, classic, tcoef\n\u001b[0;32m     77\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m [Tensor\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mt\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mt\u001b[38;5;241m.\u001b[39mdevice, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[1;32m~\\fedml-miniconda\\envs\\tinygrad\\Lib\\site-packages\\tinygrad\\nn\\optim.py:17\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, lr)\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: x\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams: List[Tensor] \u001b[38;5;241m=\u001b[39m dedup([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m params \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mrequires_grad])\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer must have at least one param\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffers: List[Tensor] \u001b[38;5;241m=\u001b[39m dedup([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m params \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mrequires_grad])   \u001b[38;5;66;03m# buffers are still realized\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: optimizer must have at least one param"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn.optim import SGD\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 10\n",
    "depth = 18\n",
    "alpha = 48\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "def load_cifar10():\n",
    "    train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "    test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train_data = [(np.array(img).transpose(2, 0, 1), label) for img, label in train_dataset]\n",
    "    test_data = [(np.array(img).transpose(2, 0, 1), label) for img, label in test_dataset]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_cifar10()\n",
    "\n",
    "# Batch Generator\n",
    "def get_batches(data, batch_size):\n",
    "    np.random.shuffle(data)\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        x = np.array([item[0] for item in batch], dtype=np.float32) / 255.0  # Normalize\n",
    "        y = np.array([item[1] for item in batch], dtype=np.int32)\n",
    "        yield Tensor(x), Tensor(y)\n",
    "\n",
    "# Model\n",
    "model = PyramidNet(in_planes=16, num_classes=num_classes, depth=depth, alpha=alpha)\n",
    "\n",
    "# Optimizer\n",
    "print(model.__dict__.values())\n",
    "optimizer = SGD([param for param in model.__dict__.values() if isinstance(param, Tensor)], lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    logits = logits - logits.max(axis=1).reshape((-1, 1))  # Stability trick\n",
    "    log_probs = logits - Tensor.log(Tensor.exp(logits).sum(axis=1)).reshape((-1, 1))\n",
    "    return -log_probs[np.arange(len(labels.data)), labels.data].mean()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in get_batches(train_data, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = cross_entropy_loss(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "for inputs, labels in get_batches(test_data, batch_size):\n",
    "    outputs = model(inputs)\n",
    "    predictions = np.argmax(outputs.numpy(), axis=1)\n",
    "    correct += (predictions == labels.numpy()).sum()\n",
    "    total += len(labels)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62176a-9e47-4041-923f-cc1c459cb7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafe541-c4cd-4b1b-92e2-3d46f8f65982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b20448-7fad-42dc-8d19-32f3424500cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9657d6-aae8-4573-b982-78243a86e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575a71c-367a-4b85-9341-98b260f5618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20452172-7d80-4ec8-9be4-d32d90790918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bda68-2405-4bf7-b16a-1d032e4d18c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b18314-00c8-414c-96b3-3bd29e35178f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
