{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4c5dbf-24c8-4e3b-b6d4-cd072e8ca5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook pyramidnet_tiny.ipynb to script\n",
      "[NbConvertApp] Writing 13356 bytes to pyramidnet_tiny.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script pyramidnet_tiny.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49434e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "import tinygrad.nn as nn\n",
    "from tinygrad.nn.datasets import cifar\n",
    "from tinygrad.nn.optim import Adam\n",
    "from tinygrad.nn.state import get_parameters\n",
    "from tinygrad.dtype import dtypes\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "import time\n",
    "from tinygrad import Device\n",
    "from tinygrad.helpers import Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371c7d29-7f40-4519-936d-790e608c2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock():\n",
    "    outchannel_ratio = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "        # eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "        # https://docs.tinygrad.org/nn/#tinygrad.nn.BatchNorm\n",
    "        # eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "        self.bn1 = nn.BatchNorm(in_channels)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        # dilation=1, groups=1\n",
    "        # https://docs.tinygrad.org/nn/#tinygrad.nn.Conv2d\n",
    "        # dilation=1, groups=1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out.relu()\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = Tensor.zeros(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1])\n",
    "            out = out + Tensor.cat(shortcut, padding, dim=1)\n",
    "        else:\n",
    "            out = out + shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck():\n",
    "    outchannel_ratio = 4\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        self.bn1 = nn.BatchNorm(in_planes)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "        # stride=1, padding=0\n",
    "        # https://docs.tinygrad.org/nn/#tinygrad.nn.BatchNorm\n",
    "        # stride=1, padding=0\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.outchannel_ratio, kernel_size=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm(planes * Bottleneck.outchannel_ratio)\n",
    "        self.relu = Tensor.relu\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out = self.bn4(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = Tensor.zeros(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1])\n",
    "            out = out + Tensor.cat(shortcut, padding, dim=1)\n",
    "        else:\n",
    "            out = out + shortcut\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PyramidNet:\n",
    "    def __init__(self, num_classes, depth, alpha, bottleneck=False):\n",
    "        if depth not in [18, 34, 50, 101, 152, 200]:\n",
    "            if bottleneck:\n",
    "                block = Bottleneck\n",
    "                temp_cfg = (depth - 2) // 12\n",
    "            else:\n",
    "                block = BasicBlock\n",
    "                temp_cfg = (depth - 2) // 8\n",
    "            layers = [temp_cfg, temp_cfg, temp_cfg, temp_cfg]\n",
    "            print('=> the layer configuration for each stage is set to', layers[depth])\n",
    "        else:\n",
    "            block = BasicBlock if depth <= 34 and not bottleneck else Bottleneck\n",
    "            if depth == 18:\n",
    "                layers = [2, 2, 2, 2]\n",
    "            elif depth in [34, 50]:\n",
    "                layers = [3, 4, 6, 3]\n",
    "            elif depth == 101:\n",
    "                layers = [3, 4, 23, 3]\n",
    "            elif depth == 152:\n",
    "                layers = [3, 8, 36, 3]\n",
    "            else:\n",
    "                layers = [3, 24, 36, 3]\n",
    "\n",
    "        self.in_planes = 64            \n",
    "        self.addrate = alpha / sum(layers)\n",
    "\n",
    "        self.input_featuremap_dim = self.in_planes\n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "\n",
    "        self.featuremap_dim = self.input_featuremap_dim \n",
    "        self.layer1 = self.pyramidal_make_layer(block, layers[0])\n",
    "        self.layer2 = self.pyramidal_make_layer(block, layers[1], stride=2)\n",
    "        self.layer3 = self.pyramidal_make_layer(block, layers[2], stride=2)\n",
    "        self.layer4 = self.pyramidal_make_layer(block, layers[3], stride=2)\n",
    "\n",
    "        self.final_featuremap_dim = self.input_featuremap_dim\n",
    "        self.bn_final = nn.BatchNorm2d(self.final_featuremap_dim)\n",
    "        self.avgpool = lambda x: x.avg_pool2d(7)\n",
    "        self.fc = nn.Linear(self.final_featuremap_dim, num_classes)\n",
    "\n",
    "    def pyramidal_make_layer(self, block, block_depth, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = lambda x: x.avg_pool2d((2, 2), stride=(2, 2), ceil_mode=True)\n",
    "\n",
    "        layers = []\n",
    "        self.featuremap_dim += self.addrate\n",
    "        layers.append(block(self.input_featuremap_dim, int(round(self.featuremap_dim)), stride, downsample))\n",
    "        for i in range(1, block_depth):\n",
    "            temp_featuremap_dim = self.featuremap_dim + self.addrate\n",
    "            layers.append(block(int(round(self.featuremap_dim)) * block.outchannel_ratio, int(round(temp_featuremap_dim)), 1))\n",
    "            self.featuremap_dim  = temp_featuremap_dim\n",
    "        self.input_featuremap_dim = int(round(self.featuremap_dim)) * block.outchannel_ratio\n",
    "        return layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = x.max_pool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        for layer in self.layer1:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer2:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer3:\n",
    "            x = layer(x)\n",
    "        for layer in self.layer4:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.bn_final(x)\n",
    "        x = x.relu()\n",
    "        x = x.avg_pool2d(7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4857aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinygradImageFolder:\n",
    "    def __init__(self, root: str, annotations_file: Optional[str] = None, class_to_idx: Optional[dict] = None, transform: Optional[list] = None):\n",
    "        self.root = root\n",
    "        self.annotations_file = annotations_file\n",
    "        self.transform = transform\n",
    "\n",
    "        if annotations_file:\n",
    "            self.class_to_idx = class_to_idx\n",
    "            self.samples = self._load_validation_annotations()\n",
    "            self.classes = None\n",
    "        else:\n",
    "            self.classes, self.class_to_idx = self._find_classes(root)\n",
    "            self.samples = self._make_dataset()\n",
    "            \n",
    "    def get_class_to_idx(self):\n",
    "        return self.class_to_idx;\n",
    "\n",
    "    def _find_classes(self, directory: str) -> Tuple[List[str], dict]:\n",
    "        classes = [d.name for d in os.scandir(directory) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def _make_dataset(self) -> List[Tuple[str, int]]:\n",
    "        instances = []\n",
    "        for class_name in self.classes:\n",
    "            class_index = self.class_to_idx[class_name]\n",
    "            class_dir = os.path.join(self.root, class_name)\n",
    "            for root, _, fnames in sorted(os.walk(class_dir)):\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    if self._is_valid_image(path):\n",
    "                        instances.append((path, class_index))\n",
    "        return instances\n",
    "\n",
    "    def _load_validation_annotations(self) -> List[Tuple[str, int]]:\n",
    "        instances = []\n",
    "        with open(self.annotations_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            \n",
    "            image_name, class_name = parts[:2]\n",
    "            image_path = os.path.join(self.root, image_name)\n",
    "\n",
    "            class_index = self.class_to_idx[class_name]\n",
    "\n",
    "            if self._is_valid_image(image_path):\n",
    "                instances.append((image_path, class_index))\n",
    "\n",
    "        return instances\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_valid_image(path: str) -> bool:\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                img.verify()\n",
    "            return True\n",
    "        except (IOError, SyntaxError):\n",
    "            return False\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, int]:\n",
    "        path, class_index = self.samples[index]\n",
    "        with Image.open(path) as img:\n",
    "            if self.transform:\n",
    "                for t in self.transform:\n",
    "                    img = t(img)\n",
    "        return Tensor(img), Tensor(class_index)\n",
    "\n",
    "\n",
    "class TinygradDataLoader:\n",
    "    def __init__(self, dataset: TinygradImageFolder, batch_size: int, shuffle: bool):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        if shuffle:\n",
    "            self.shuffle_array = np.arange(len(dataset))\n",
    "            np.random.shuffle(self.shuffle_array)\n",
    "        self.current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "        \n",
    "        begin = self.current\n",
    "        end = self.current + self.batch_size\n",
    "        if end > len(self.dataset):\n",
    "            end = len(self.dataset)\n",
    "        if self.shuffle == True:\n",
    "            x = [self.dataset[i][0]\n",
    "                 for i in self.shuffle_array[begin:end]]\n",
    "            y = [self.dataset[i][1]\n",
    "                 for i in self.shuffle_array[begin:end]]\n",
    "        else:\n",
    "            x = [self.dataset[i][0]\n",
    "                for i in range(begin, end)]\n",
    "            y = [self.dataset[i][1]\n",
    "                for i in range(begin, end)]\n",
    "        self.current += self.batch_size\n",
    "        return Tensor.stack(*x), Tensor.stack(*y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4674ec71-c6a8-4f99-8b9d-6c350b20bb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [21:59,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.887159824371338 | Accuracy: 0.109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [30:48,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.65981912612915 | Accuracy: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [39:36,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.6744771003723145 | Accuracy: 0.078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [48:25,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.475789546966553 | Accuracy: 0.078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [57:16,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.310274600982666 | Accuracy: 0.078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [57:19,  6.88s/it]\n",
      "157it [02:39,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026970541401273886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(Device.DEFAULT)\n",
    "\n",
    "num_classes = 200\n",
    "depth = 18\n",
    "alpha = 48\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "transforms = [\n",
    "    lambda x: x.resize((224, 224)),\n",
    "    lambda x: x.convert(\"RGB\"),\n",
    "    lambda x: np.array(x).transpose((2, 0, 1))\n",
    "]\n",
    "\n",
    "train_dataset = TinygradImageFolder(\n",
    "    root='C:\\\\Users\\\\elect\\\\Documents\\\\SEM9\\\\Advanced Machine Learning\\\\tiny-imagenet-200\\\\train', transform=transforms)\n",
    "\n",
    "val_dataset = TinygradImageFolder(\n",
    "    root='C:\\\\Users\\\\elect\\\\Documents\\\\SEM9\\\\Advanced Machine Learning\\\\tiny-imagenet-200\\\\val\\\\images', annotations_file=\"C:/Users/elect/Documents/SEM9/Advanced Machine Learning/tiny-imagenet-200/val/val_annotations.txt\", class_to_idx=train_dataset.get_class_to_idx(), transform=transforms)\n",
    "\n",
    "\n",
    "train_loader = TinygradDataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = TinygradDataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = PyramidNet(num_classes=num_classes, depth=depth, alpha=alpha, bottleneck=True)\n",
    "\n",
    "optimizer = Adam(get_parameters(model), lr=learning_rate)\n",
    "\n",
    "with Tensor.train():\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "        time1 = time.time()\n",
    "        \n",
    "        out = model(images)\n",
    "        time2 = time.time()\n",
    "        \n",
    "        loss = out.sparse_categorical_crossentropy(labels)\n",
    "        time3 = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        time4 = time.time()\n",
    "        \n",
    "        loss.backward()\n",
    "        time5 = time.time()\n",
    "        \n",
    "        optimizer.step()\n",
    "        time6 = time.time()\n",
    "\n",
    "        if i == 500:\n",
    "            break\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            pred = out.argmax(axis=-1)\n",
    "            acc = (pred == labels).mean()\n",
    "            print(f\"Loss: {loss.numpy()} | Accuracy: {acc.numpy()}\")\n",
    "\n",
    "with Tensor.test():\n",
    "    acc = []\n",
    "    for val_images, val_labels in tqdm(val_loader):\n",
    "        out = model(val_images)\n",
    "        pred = out.argmax(axis=-1)\n",
    "        acc.append((pred == val_labels).mean().numpy())\n",
    "    print(sum(acc) / len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9b43a-78ab-4ad4-b51e-c99a0ef6b819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99841f8b-ce56-46fd-b35d-6ec19aed160c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd8aba-0f48-47a0-9bbd-9f9340975bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
